{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "b0fead08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "import math\n",
    "from torch import linalg as LA\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "2d570211",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contrastive Loss Module (old/inefficient functions—they are correct but use things like for loops instead of vectorization)\n",
    "\n",
    "# Define the similarity function: Dot product divided by L2 norm of vectors, with temperature term that is default to 1\n",
    "\n",
    "# sim(i, j) = torch.dot(i, j) / T * |i| * |j|\n",
    "\n",
    "# My sim function incorporates a Temperature constant that can be changed. For regular cosine sim, temp = 1.\n",
    "\n",
    "def sim(i, j, T=1):\n",
    "    sim = torch.dot(i, j) / (T * LA.vector_norm(i) * LA.vector_norm(j)) \n",
    "    return sim\n",
    "\n",
    "\n",
    "# pair_loss = sim(i, j) / Σ(k = 0--> 2N - 1, k != i)[sim(i, k)] \n",
    "\n",
    "def pair_loss(images_tensor, i, j):\n",
    "    # Defining the numerator and denominator terms\n",
    "    numer = torch.exp(sim(images_tensor[i], images_tensor[j])).float()\n",
    "    denom = torch.zeros(1)\n",
    "    \n",
    "    # Calculating the denominator term\n",
    "    for index in range(len(images_tensor)):\n",
    "        if index == i:\n",
    "            continue\n",
    "        else:\n",
    "            denom += torch.exp(sim(images_tensor[i], images_tensor[index]))\n",
    "            \n",
    "    # Combining the numerator and denominator terms, and finally applying the -log to get loss\n",
    "    contr_loss = numer / denom\n",
    "    contr_loss = -torch.log(contr_loss)\n",
    "    return contr_loss\n",
    "\n",
    "def new_pair_loss(sim_matrix, i, j):\n",
    "    # Defining the numerator and denominator terms\n",
    "    numer = torch.exp(sim_matrix[i, j]).float()\n",
    "    denom = torch.exp(sim_matrix[i]).sum() - torch.exp(sim_matrix[i, i])\n",
    "            \n",
    "    # Combining the numerator and denominator terms, and finally applying the -log to get loss\n",
    "    contr_loss = numer / denom\n",
    "    contr_loss = -torch.log(contr_loss)\n",
    "    return contr_loss\n",
    "\n",
    "\n",
    "# contrastive_loss = (1/number_of_images) * Σ(i = 0 --> 2N - 2)[pair_loss(images_tensor, i, i + 1) + pair_loss(images_tensor, i + 1, i)]\n",
    "\n",
    "def total_loss(images_tensor):\n",
    "    batch_size = len(images_tensor) # instead of doing 1 / 2 * num_pairs, we can just do 1 / batch_size\n",
    "    first_term = 1 / batch_size\n",
    "    summation_term = torch.zeros(1)\n",
    "    # Calculating the summation term\n",
    "    for index in range(0, len(images_tensor), 2):\n",
    "        summation_term += pair_loss(images_tensor, index, index + 1) + pair_loss(images_tensor, index + 1, index)\n",
    "    total_loss = first_term * summation_term\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "db3634cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(-1.), tensor(0.), tensor(0.0156))"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Checks\n",
    "t = torch.ones(5) \n",
    "u = torch.ones(5)\n",
    "test = sim(t, u)               \n",
    "\n",
    "i = torch.ones(5) * -1\n",
    "j = torch.ones(5)\n",
    "test1 = sim(i, j)\n",
    "\n",
    "h = torch.tensor((0,1)).float()\n",
    "k = torch.tensor((1, 0)).float()\n",
    "test2 = sim(h, k)\n",
    "\n",
    "r = torch.randn(10).float()\n",
    "s = torch.randn(10).float()\n",
    "test3 = sim(r, s)\n",
    "\n",
    "test, test1, test2, test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "bff0afb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0000), tensor(-1.0000), tensor(0.), tensor(0.0156))"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing with PyTorch CosineSimilarity\n",
    "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6) # For 1-D tensors, we compute cosine similarity across dim=0\n",
    "testPy = cos(t, u)\n",
    "test1Py = cos(i, j)\n",
    "test2Py = cos(h, k)\n",
    "test3Py = cos(r, s)\n",
    "testPy, test1Py, test2Py, test3Py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "e671d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out pair_loss. First I make a list of tensors to pass into the pair_loss function\n",
    "ls = [t, u, i, j] # Putting tensors all into one list\n",
    "ls_tensor = torch.stack(ls, dim=0) # Stacking the tensors into a new tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "661c260c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7586]), tensor([0.7586]))"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contr_test = pair_loss(ls, 0, 1)\n",
    "contr_test_tensor = pair_loss(ls_tensor, 0, 1)\n",
    "contr_test, contr_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "ab320285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3436])"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing out the total_loss function. \n",
    "tot_loss = total_loss(ls)\n",
    "tot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "506a4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing more sanity checks on the total_loss function. First case will be high loss case, second case will be low loss case\n",
    "a = torch.tensor((-1, -1)).float()\n",
    "b = torch.tensor((1, 1)).float()\n",
    "c = torch.tensor((-1, -1)).float()\n",
    "d = torch.tensor((1, 1)).float()\n",
    "\n",
    "w = torch.tensor((1, 1)).float()\n",
    "x = torch.tensor((1, 1)).float()\n",
    "y = torch.tensor((-1, -1)).float()\n",
    "z = torch.tensor((-1, -1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "d6e62860",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_high_loss = [a, b, c, d]\n",
    "ls_low_loss = [w, x, y, z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "ffad23cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_loss = total_loss(ls_high_loss)\n",
    "low_loss = total_loss(ls_low_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "abc3eb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2395])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "9533b217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2395])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "152d0a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the expected result that the low_loss case, which has positive pairs that are similar to each other and \n",
    "# dissimilar from the rest, has a significantly lower loss than the high_loss case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "6c6d2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_high_loss_tensor = torch.stack(ls_high_loss, dim=0)\n",
    "ls_low_loss_tensor = torch.stack(ls_low_loss, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "6db742d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_loss_tensor = total_loss(ls_high_loss_tensor)\n",
    "low_loss_tensor = total_loss(ls_low_loss_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "0aa508f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2395])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_loss_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "eebdf29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2395])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_loss_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "5e5b1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final versions of Contrastive Loss Functions\n",
    "\n",
    "# Pre-compute all similarities to index into later. T = temperature term \n",
    "def sim_matrix(images_tensor, T=1):\n",
    "    sim_mat = torch.matmul(images_tensor, images_tensor.T) / (T * torch.matmul(LA.vector_norm(images_tensor, dim=1, ord=2, keepdim=True), \n",
    "                                                                               LA.vector_norm(images_tensor.T, dim=0, ord=2, keepdim=True)))\n",
    "    return sim_mat\n",
    "\n",
    "# Lots of un-needed information here--only care about pair_loss for the positive pairs(0,1 2,3 4,5 etc)\n",
    "def pair_loss_matrix(sim_matrix):\n",
    "    numer = torch.exp(sim_matrix).float()\n",
    "    denom_row = torch.exp(sim_matrix).sum(dim=1) \n",
    "    denom_ii = torch.exp(torch.diagonal(sim_matrix, 0)).float()\n",
    "    denom = denom_row.sub(denom_ii.reshape(1, -1))\n",
    "    \n",
    "    pair_loss_matrix = numer / denom\n",
    "    pair_loss_matrix = -torch.log(pair_loss_matrix)\n",
    "    pair_loss_matrix = pair_loss_matrix.T \n",
    "    return pair_loss_matrix\n",
    "\n",
    "def total_contrastive_loss(pair_loss_matrix):\n",
    "    batch_size = len(pair_loss_matrix)\n",
    "    first_term = 1 / batch_size\n",
    "    summation_term = torch.zeros(1)\n",
    "    \n",
    "    # Calculating summation term (I don't know how to get rid of this for-loop)\n",
    "    for i in range(0, len(pair_loss_matrix), 2):\n",
    "        summation_term += pair_loss_matrix[i, i+1] + pair_loss_matrix[i+1, i]\n",
    "    \n",
    "    total_contrastive_loss = first_term * summation_term\n",
    "    return total_contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "6fe5d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in matrix form, get it working with a small CNN(grayscale), \n",
    "\n",
    "# 4 images, global average pool to get vector representation\n",
    "# 4 x 1 x 12 x 12 (2D convolutions expect a channel representation)\n",
    "# unsqueeze for input\n",
    "# .squeeze\n",
    "# pytorch.conv2D\n",
    "# 2 convolutions then pool \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "4cc440d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing my test images/tensors\n",
    "\n",
    "a = torch.randn(4, 1, 32, 32) # This represents a tensor with 4 images, each\n",
    "# is 12x12, with one channel(Conv2D) expects a channel representation.\n",
    "a[0] = torch.ones(1, 32, 32)\n",
    "a[1] = a[0] * 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "55424217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tensors coming from normal distribution \n",
    "c0 = torch.normal(mean=0, std=1, size=(32,32))\n",
    "c1 = torch.normal(mean=0, std=1, size=(32,32))\n",
    "c2 = torch.normal(mean=1, std=1, size=(32,32))\n",
    "c3 = torch.normal(mean=1, std=1, size=(32,32))\n",
    "c = torch.stack((c0, c1, c2, c3), dim=0)\n",
    "c = torch.unsqueeze(c, 1) # Adding in a dimension because the ConvNet expects a dimension for channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "aafae90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 32, 32])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape # This is the correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "221b595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test0 = c0.view(1, -1).squeeze()\n",
    "test1 = c1.view(1, -1).squeeze()\n",
    "test2 = c2.view(1, -1).squeeze()\n",
    "test3 = c3.view(1, -1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "a1cc18ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0481)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim(test0, test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "23dbf437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 5) # 1 input channel, 3 output, 5x5 kernel_size\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(3, 16, 5) # input channel = 3, output = 16, kernel_size = 5x5\n",
    "        self.fc1 = nn.Linear(16*5*5, 80) # Converting into linear layer\n",
    "        self.fc2 = nn.Linear(80, 40)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "8c782c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the ConvNet Model\n",
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "58c2ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing hyperparameters and choice of optimizer, etc\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "685f4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a training loop example, update the network, and see how loss changes over time.\n",
    "# Use random normal distributions. Have 2 with mean = 1, and 2 with mean = 0\n",
    "\n",
    "outputs = model(c)\n",
    "simi_matrix = sim_matrix(outputs)\n",
    "pair_matrix = pair_loss_matrix(simi_matrix)\n",
    "loss = total_contrastive_loss(pair_matrix)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "80707522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2989], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "442e2ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.9936, -0.7556, -0.7555],\n",
       "        [ 0.9936,  1.0000, -0.7518, -0.7513],\n",
       "        [-0.7556, -0.7518,  1.0000,  0.9948],\n",
       "        [-0.7555, -0.7513,  0.9948,  1.0000]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "a5298976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2629,  0.0681, -0.0458,  0.2901,  0.0620, -0.1511, -0.1972, -0.1514,\n",
       "          0.0325, -0.2544, -0.0863, -0.1450, -0.0695, -0.0087, -0.2168,  0.0933,\n",
       "         -0.0544, -0.2334, -0.2493, -0.0604,  0.1137,  0.0458, -0.0370,  0.1339,\n",
       "         -0.1678, -0.0253,  0.1009,  0.1556, -0.0202,  0.1524, -0.1143,  0.0019,\n",
       "          0.1305, -0.0919,  0.2155,  0.0052,  0.1132,  0.0717,  0.1444,  0.0371],\n",
       "        [-0.2642,  0.0097, -0.0591,  0.2470,  0.0571, -0.2071, -0.2574, -0.2841,\n",
       "         -0.0575, -0.3616, -0.0820, -0.1122, -0.1293, -0.0222, -0.1608,  0.0834,\n",
       "         -0.0235, -0.2164, -0.1427, -0.0820,  0.1317,  0.0004,  0.0323,  0.1402,\n",
       "         -0.1589, -0.0373,  0.0077,  0.1627, -0.0200,  0.1851, -0.1127, -0.0360,\n",
       "          0.1606, -0.1819,  0.2623,  0.1147,  0.1427,  0.0270,  0.1122,  0.0993],\n",
       "        [-0.2402,  0.0808, -0.0065,  0.2973, -0.0603, -0.2012, -0.2092, -0.2340,\n",
       "         -0.0126, -0.2514,  0.0758, -0.0837, -0.1252, -0.0264, -0.2593,  0.0217,\n",
       "          0.0435, -0.1101, -0.0676,  0.0174,  0.1799,  0.0199,  0.0484,  0.0613,\n",
       "         -0.1227, -0.0373, -0.0342,  0.1473,  0.0038,  0.1799, -0.0365,  0.0304,\n",
       "          0.1542, -0.0845,  0.2608,  0.0513,  0.1770,  0.1042,  0.1165,  0.1409],\n",
       "        [-0.2606,  0.1310, -0.0347,  0.2713, -0.0940, -0.2597, -0.2044, -0.2322,\n",
       "         -0.0646, -0.2128,  0.0222, -0.0629, -0.0980, -0.0088, -0.2220,  0.1115,\n",
       "         -0.0151, -0.1940, -0.0776, -0.0091,  0.2114,  0.0324,  0.0127,  0.0721,\n",
       "         -0.1402,  0.0119, -0.1015,  0.1179, -0.0038,  0.1879,  0.0115,  0.0344,\n",
       "          0.2408, -0.0843,  0.2816,  0.0948,  0.1906,  0.1153,  0.0889,  0.1247]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "5197be82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.4442e+00,  1.7939e+00, -1.6226e+00,  ..., -7.4788e-01,\n",
       "           -1.3616e+00,  1.9252e-01],\n",
       "          [-7.3228e-01, -7.7949e-01,  2.3209e-01,  ...,  4.4246e-01,\n",
       "            1.3686e+00, -8.8945e-01],\n",
       "          [-2.8290e-02,  1.6922e+00,  9.5502e-01,  ...,  1.2939e+00,\n",
       "            1.1230e+00, -3.1070e-01],\n",
       "          ...,\n",
       "          [-6.9299e-01, -7.1403e-01,  1.0624e+00,  ..., -2.4580e-01,\n",
       "            1.7204e-01, -1.6623e+00],\n",
       "          [ 1.4962e+00, -1.1881e+00,  1.4085e+00,  ...,  2.4769e-02,\n",
       "           -1.1320e+00, -1.1743e+00],\n",
       "          [ 1.2182e+00, -1.7938e+00,  6.4066e-01,  ..., -7.4276e-01,\n",
       "            1.0765e+00,  3.2862e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.5773e-01,  2.2596e+00,  2.4385e-01,  ...,  2.1024e+00,\n",
       "            1.3358e-01,  1.2560e+00],\n",
       "          [ 3.4708e-01, -9.8906e-02,  7.0650e-01,  ...,  1.0119e+00,\n",
       "            9.2819e-01, -3.7837e-01],\n",
       "          [ 3.4117e-01, -1.7430e-02, -2.5657e-02,  ..., -1.1083e+00,\n",
       "            7.9414e-01,  1.9632e+00],\n",
       "          ...,\n",
       "          [-1.4915e+00,  1.3158e-02, -5.1646e-02,  ..., -7.4285e-01,\n",
       "            9.4583e-01,  1.5714e+00],\n",
       "          [-1.9774e+00,  5.7820e-01, -7.8166e-01,  ...,  5.5903e-01,\n",
       "           -4.8782e-01,  3.5467e-01],\n",
       "          [-8.4039e-01, -1.2895e-01,  4.9598e-01,  ..., -2.5825e-03,\n",
       "            8.9640e-01,  5.8702e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 9.1477e-02,  4.9308e-01,  1.6608e+00,  ...,  1.0090e+00,\n",
       "            2.5263e+00,  2.4505e+00],\n",
       "          [ 1.1475e+00,  5.8573e-01,  6.8240e-01,  ...,  1.4307e+00,\n",
       "           -1.4333e+00,  2.2019e+00],\n",
       "          [ 1.2904e+00,  5.9809e-01, -3.3929e-01,  ...,  9.5637e-01,\n",
       "            7.0689e-01,  2.2431e-01],\n",
       "          ...,\n",
       "          [ 5.0748e-01,  2.4634e+00,  1.2421e+00,  ...,  3.0846e-01,\n",
       "            1.4474e+00,  1.9810e+00],\n",
       "          [ 2.4418e+00, -2.6610e-01,  9.8995e-01,  ...,  1.7580e+00,\n",
       "            2.1685e+00,  9.7878e-01],\n",
       "          [ 2.5846e+00,  1.3168e+00,  1.5162e+00,  ...,  3.7956e-01,\n",
       "            1.3625e+00,  1.4239e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9463e+00,  1.7213e+00,  1.3824e+00,  ...,  8.3064e-01,\n",
       "            1.2532e-01,  9.3733e-01],\n",
       "          [ 9.2991e-01,  3.8514e-01,  1.1202e+00,  ...,  1.6539e+00,\n",
       "            1.9178e+00,  1.4304e+00],\n",
       "          [ 2.1525e-02,  5.3828e-01, -3.1330e-01,  ...,  2.9353e+00,\n",
       "            5.8560e-02,  2.2177e+00],\n",
       "          ...,\n",
       "          [ 1.0227e+00,  6.9959e-01,  6.7012e-01,  ...,  1.4003e+00,\n",
       "            1.6755e-01,  1.7741e+00],\n",
       "          [ 3.4884e-01, -4.1576e-01,  2.1839e+00,  ...,  3.9987e-01,\n",
       "            5.5354e-01,  8.2014e-02],\n",
       "          [ 4.9811e-01,  2.5984e+00,  5.1481e-01,  ...,  1.4490e+00,\n",
       "            2.3378e+00,  6.4174e-01]]]])"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "d1281b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2921, 0.2985, 2.0477, 2.0476],\n",
       "        [0.2995, 0.2932, 2.0450, 2.0444],\n",
       "        [2.0491, 2.0453, 0.2935, 0.2987],\n",
       "        [2.0491, 2.0448, 0.2988, 0.2935]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "c7b9421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into proper training loop(prioritize)\n",
    "\n",
    "# Functions: predict(calls model(x)), train_one_step, train_one_epoch, train\n",
    "\n",
    "# Switch gears into dataloader (pseudocode first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "aaee451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model):\n",
    "    outputs = model(data)\n",
    "    return outputs\n",
    "\n",
    "def train_one_step(batch, model, optimizer):\n",
    "    outputs = predict(batch, model)\n",
    "    simi_matrix = sim_matrix(outputs)\n",
    "    pair_matrix = pair_loss_matrix(simi_matrix)\n",
    "    loss = total_contrastive_loss(pair_matrix)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    " \n",
    "def train_one_epoch(dataloader, model, optimizer): # Take in dataloader\n",
    "    for batch in dataloader:\n",
    "        currentLoss = train_one_step(batch, model, optimizer)\n",
    "        random.shuffle(dataloader) # shuffle the dataloader for training\n",
    "    return currentLoss\n",
    "\n",
    "def train(dataloader, model, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        trainingLoss = train_one_epoch(dataloader, model, optimizer)\n",
    "    return trainingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "ee6a7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct batch, copy it and put into list, then this is dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "fd59ad73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0899,  0.0754,  0.2073,  0.1371, -0.2054,  0.1405, -0.1768, -0.1879,\n",
       "          0.1793, -0.0022,  0.0676,  0.0168,  0.1717,  0.0535,  0.0563,  0.1734,\n",
       "         -0.0268,  0.2787, -0.0393, -0.0943, -0.1901,  0.0461, -0.0285, -0.0453,\n",
       "         -0.2318,  0.0895, -0.0535, -0.0448,  0.0019, -0.0919,  0.1602, -0.1038,\n",
       "         -0.1382, -0.2871, -0.2087, -0.1387,  0.0241,  0.0600,  0.1741,  0.2057],\n",
       "        [-0.1115,  0.1133,  0.2121,  0.1573, -0.1732,  0.1522, -0.1369, -0.1641,\n",
       "          0.1176, -0.0495,  0.0854,  0.0180,  0.1427,  0.1012,  0.0474,  0.2207,\n",
       "         -0.0076,  0.2919, -0.0223, -0.0317, -0.1766,  0.0648,  0.0331, -0.0787,\n",
       "         -0.1565,  0.0978, -0.0820, -0.1387, -0.0033, -0.0718,  0.1993, -0.0794,\n",
       "         -0.0733, -0.2748, -0.1965, -0.0796,  0.0931,  0.1266,  0.1559,  0.1854],\n",
       "        [-0.0582,  0.0375,  0.1223,  0.0973, -0.1881,  0.1248, -0.0802, -0.1989,\n",
       "          0.0731, -0.0773,  0.1191, -0.0032,  0.0825,  0.0677,  0.0333,  0.1184,\n",
       "          0.0268,  0.2249, -0.0879, -0.0977, -0.1542,  0.0717, -0.0179, -0.0028,\n",
       "         -0.1887,  0.0956, -0.0301, -0.0472,  0.0087, -0.0415,  0.0895, -0.0784,\n",
       "         -0.1054, -0.1952, -0.1479, -0.0475, -0.0205, -0.0018,  0.0868,  0.1705],\n",
       "        [-0.0778,  0.0879,  0.1435,  0.1554, -0.0975,  0.1185, -0.1703, -0.1986,\n",
       "          0.0799, -0.0369,  0.0889, -0.0211,  0.0871,  0.0426,  0.0303,  0.1543,\n",
       "          0.0175,  0.1679, -0.0864, -0.0769, -0.1277,  0.1144, -0.0699,  0.0113,\n",
       "         -0.0983,  0.0891, -0.0515, -0.1859, -0.0139,  0.0023,  0.1634, -0.0905,\n",
       "         -0.0299, -0.1955, -0.1627, -0.0783,  0.0958,  0.0806,  0.0738,  0.1869]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing predict function\n",
    "predict(dataloader[0], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "eda05dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tensors coming from normal distribution \n",
    "t0 = torch.normal(mean=0, std=1, size=(32,32))\n",
    "t1 = torch.normal(mean=0, std=1, size=(32,32))\n",
    "t2 = torch.normal(mean=1, std=1, size=(32,32))\n",
    "t3 = torch.normal(mean=1, std=1, size=(32,32))\n",
    "t = torch.stack((t0, t1, t2, t3), dim=0)\n",
    "t = torch.unsqueeze(t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "d4c8ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = torch.normal(mean=0, std=1, size=(32,32))\n",
    "u1 = torch.normal(mean=0, std=1, size=(32,32))\n",
    "u2 = torch.normal(mean=1, std=1, size=(32,32))\n",
    "u3 = torch.normal(mean=1, std=1, size=(32,32))\n",
    "u = torch.stack((u0, u1, u2, u3), dim=0)\n",
    "u = torch.unsqueeze(u, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "1240a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "v0 = torch.normal(mean=0, std=1, size=(32,32))\n",
    "v1 = torch.normal(mean=0, std=1, size=(32,32))\n",
    "v2 = torch.normal(mean=1, std=1, size=(32,32))\n",
    "v3 = torch.normal(mean=1, std=1, size=(32,32))\n",
    "v = torch.stack((v0, v1, v2, v3), dim=0)\n",
    "v = torch.unsqueeze(v, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "f21b16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = torch.normal(mean=0, std=1, size=(32,32))\n",
    "w1 = torch.normal(mean=0, std=1, size=(32,32))\n",
    "w2 = torch.normal(mean=1, std=1, size=(32,32))\n",
    "w3 = torch.normal(mean=1, std=1, size=(32,32))\n",
    "w = torch.stack((w0, w1, w2, w3), dim=0)\n",
    "w = torch.unsqueeze(w, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "28c4077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = [t, u, v, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "d6d12ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testModel = ConvNet()\n",
    "test_learning_rate = 0.1\n",
    "testOptimizer = torch.optim.SGD(testModel.parameters(), lr=test_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "4e8472b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0799], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_step(dataloader[0], testModel, testOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "f09d25dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6643], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_epoch(dataloader, testModel, testOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "37e6d5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2396], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(dataloader, testModel, testOptimizer, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
